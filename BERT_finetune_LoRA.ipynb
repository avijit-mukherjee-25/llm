{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMmzMwM9V1HpoYK/qpql37X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avijit-mukherjee-25/llm/blob/main/BERT_finetune_LoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNntWkr03bp5",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# install libraries\n",
        "!pip install transformers torch numpy datasets evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft"
      ],
      "metadata": {
        "id": "jCX7QdL4Xnu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget"
      ],
      "metadata": {
        "id": "16jmyXod4DUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, datasets\n",
        "import torch"
      ],
      "metadata": {
        "id": "cLmSEQ3u3QEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "opWKSz5m3UfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Data"
      ],
      "metadata": {
        "id": "w2k_k-NB7jSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wget\n",
        "import os\n",
        "\n",
        "print('Downloading dataset...')\n",
        "\n",
        "# The URL for the dataset zip file.\n",
        "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
        "\n",
        "# Download the file (if we haven't already)\n",
        "if not os.path.exists('./cola_public_1.1.zip'):\n",
        "    wget.download(url, './cola_public_1.1.zip')"
      ],
      "metadata": {
        "id": "jbzxDTQo4o13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the dataset (if we haven't already)\n",
        "if not os.path.exists('./cola_public/'):\n",
        "    !unzip cola_public_1.1.zip"
      ],
      "metadata": {
        "id": "oN5a6vOW44Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "df = df[['label','sentence']].copy()\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "metadata": {
        "id": "IiXzHj405B0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "met1xs1X5IDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(1337)\n",
        "idx = np.random.random(size=df.shape[0])<0.8\n",
        "df_train = df.iloc[idx]\n",
        "df_test = df.iloc[~idx]\n",
        "print (df_train.shape, df_test.shape)"
      ],
      "metadata": {
        "id": "TqjukQnt5ztu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Data\n",
        "\n",
        "Approach 1"
      ],
      "metadata": {
        "id": "9oHpVS9R3rAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\", do_lower_case=True)\n",
        "\n",
        "# # Alternatively, we could have used BertTokenizer directly\n",
        "# from transformers import BertTokenizer\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "metadata": {
        "id": "FlNgZbWv3nV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the original sentence.\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))\n",
        "\n",
        "# encode\n",
        "print (tokenizer.encode(sentences[0], add_special_tokens=True))\n",
        "\n",
        "# special tokens\n",
        "print (tokenizer.convert_ids_to_tokens(101), tokenizer.convert_ids_to_tokens(102))\n",
        "\n",
        "print (\n",
        "    tokenizer.encode_plus(\n",
        "        sentences[0],\n",
        "        add_special_tokens=True,\n",
        "        max_length=64,\n",
        "        pad_to_max_length=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "t8nRjFV44Awh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)"
      ],
      "metadata": {
        "id": "pe9-iFDI4EKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set max_len to 64\n",
        "max_len = 64\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = max_len,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "\n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "metadata": {
        "id": "mde0lcwH4Gq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Data\n",
        "\n",
        "Approach 2 (Using Transformers Dataset class)"
      ],
      "metadata": {
        "id": "cOksluRJ4Mm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "train_data = Dataset.from_pandas(df_train)\n",
        "# # alternatively we could have using from_dict as follows\n",
        "# data = Dataset.from_dict({\"sentence\": sentences, \"label\": labels})\n",
        "train_data"
      ],
      "metadata": {
        "id": "13szKz-A4JqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = Dataset.from_pandas(df_test)\n",
        "test_data"
      ],
      "metadata": {
        "id": "UockQ3gd4Ztg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[0], test_data[0]"
      ],
      "metadata": {
        "id": "vqOC2UdW4cfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(_data):\n",
        "    return tokenizer(_data['sentence'], padding=\"max_length\", truncation=True)"
      ],
      "metadata": {
        "id": "E-OkIg1B4eRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_data.map(tokenize_function, batched=True)\n",
        "test_data = test_data.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "85QaRxKN4h8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_data.rename_columns({\"label\": \"labels\"})\n",
        "test_data = test_data.rename_column(\"label\", \"labels\")\n",
        "train_data, test_data"
      ],
      "metadata": {
        "id": "o_tIOVl44kJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train using Pytorch Trainer"
      ],
      "metadata": {
        "id": "x8YDEV7d4m0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "5eD-9Uux4w2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-uncased\", num_labels=2, torch_dtype=\"auto\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "ZPdk6K5z4l_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "metadata": {
        "id": "aTOXqbTE4sXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer"
      ],
      "metadata": {
        "id": "hTjghICh41eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "g-87IPf343ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "4yQKuNuX46ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='test_trainer',\n",
        "    eval_strategy=\"epoch\",\n",
        "    num_train_epochs=2,\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "metadata": {
        "id": "G6GlGQM25Le8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=test_data,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "1BPT_vYJ5PYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.train()"
      ],
      "metadata": {
        "id": "Zi7e__KB5igS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetune using native Pytorch"
      ],
      "metadata": {
        "id": "M6NH5KOUSO12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate.utils.memory import clear_device_cache\n",
        "del model\n",
        "del trainer\n",
        "clear_device_cache()"
      ],
      "metadata": {
        "id": "QL8pbt605kBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.set_format('torch')\n",
        "test_data.set_format('torch')"
      ],
      "metadata": {
        "id": "cyM5W1x4ST_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataloader\n",
        "from torch.utils.data import DataLoader\n",
        "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=32)\n",
        "test_dataloader = DataLoader(test_data, shuffle=True, batch_size=32)"
      ],
      "metadata": {
        "id": "wGfBP7XrSVqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-uncased\", num_labels=2)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "D1WO_VF6SXTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "OG4EoEDYSZEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_scheduler\n",
        "num_epochs = 2\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        ")"
      ],
      "metadata": {
        "id": "Auldm0ZtSc1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, lr_scheduler, train_dataloader):\n",
        "    train_loss = 0.0\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        input_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        result = model(input_ids,\n",
        "               token_type_ids=None,\n",
        "               attention_mask=input_mask,\n",
        "               labels=labels,\n",
        "               return_dict=True)\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if step%100==0:\n",
        "            print (f'training loss at {step} is {train_loss}')\n",
        "    return train_loss"
      ],
      "metadata": {
        "id": "EF6_WcAPSfaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval(model, test_dataloader):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    for _, batch in enumerate(test_dataloader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        input_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        result = model(input_ids,\n",
        "               token_type_ids=None,\n",
        "               attention_mask=input_mask,\n",
        "               labels=labels,\n",
        "               return_dict=True)\n",
        "        loss = result.loss\n",
        "        val_loss += loss.item()\n",
        "        logits = result.logits\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        metric.add_batch(predictions=predictions, references=labels)\n",
        "    print (f'validation loss is {val_loss}')\n",
        "    val_accuracy = metric.compute()\n",
        "    model.train()\n",
        "    return val_accuracy"
      ],
      "metadata": {
        "id": "n6A5PWTXShJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for epoch in range(num_epochs):\n",
        "#     train_loss = train(model, optimizer, lr_scheduler, train_dataloader)\n",
        "#     print (f'train loss at epoch {epoch} is {train_loss}')\n",
        "#     val_accuracy = eval(model, test_dataloader)\n",
        "#     print (f'validation accuracy at epoch {epoch} is {val_accuracy}')"
      ],
      "metadata": {
        "id": "yKYcTqYRSi2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetune (native PyTorch) BERT using LoRA"
      ],
      "metadata": {
        "id": "jNWUpapJYzLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from accelerate.utils.memory import clear_device_cache\n",
        "# del model\n",
        "# del trainer\n",
        "# clear_device_cache()"
      ],
      "metadata": {
        "id": "R9Ygnzz9SlO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.set_format('torch')\n",
        "test_data.set_format('torch')"
      ],
      "metadata": {
        "id": "SDQwok79Y82C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataloader\n",
        "from torch.utils.data import DataLoader\n",
        "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=32)\n",
        "test_dataloader = DataLoader(test_data, shuffle=True, batch_size=32)"
      ],
      "metadata": {
        "id": "6YPp-XR4Y_wO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, TaskType,  get_peft_config, get_peft_model"
      ],
      "metadata": {
        "id": "LHy1VIlTZCEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(task_type=TaskType.SEQ_CLS, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)"
      ],
      "metadata": {
        "id": "x0Of-UVqZH-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-uncased\", num_labels=2, torch_dtype=\"auto\")\n",
        "model = get_peft_model(model, peft_config)"
      ],
      "metadata": {
        "id": "WYNaU6z_ZKSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "TiasDT_oZMiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "id": "CSIoYQDBZOoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)"
      ],
      "metadata": {
        "id": "_jnt7pyhZQKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_scheduler\n",
        "num_epochs = 2\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        ")"
      ],
      "metadata": {
        "id": "APQ5ExlJZSVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, lr_scheduler, train_dataloader):\n",
        "    train_loss = 0.0\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        input_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        result = model(input_ids,\n",
        "               token_type_ids=None,\n",
        "               attention_mask=input_mask,\n",
        "               labels=labels,\n",
        "               return_dict=True)\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if step%100==0:\n",
        "            print (f'training loss at {step} is {train_loss}')\n",
        "    return train_loss"
      ],
      "metadata": {
        "id": "NTcapVzoZT-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval(model, test_dataloader):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    for _, batch in enumerate(test_dataloader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        input_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        result = model(input_ids,\n",
        "               token_type_ids=None,\n",
        "               attention_mask=input_mask,\n",
        "               labels=labels,\n",
        "               return_dict=True)\n",
        "        loss = result.loss\n",
        "        val_loss += loss.item()\n",
        "        logits = result.logits\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        metric.add_batch(predictions=predictions, references=labels)\n",
        "    print (f'validation loss is {val_loss}')\n",
        "    val_accuracy = metric.compute()\n",
        "    model.train()\n",
        "    return val_accuracy"
      ],
      "metadata": {
        "id": "zSvisqAfZWO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, optimizer, lr_scheduler, train_dataloader)\n",
        "    print (f'train loss at epoch {epoch} is {train_loss}')\n",
        "    val_accuracy = eval(model, test_dataloader)\n",
        "    print (f'validation accuracy at epoch {epoch} is {val_accuracy}')"
      ],
      "metadata": {
        "id": "1hgKbwMPZX1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetune BERT-LoRA using Trainer"
      ],
      "metadata": {
        "id": "v5_FynLhdoaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from accelerate.utils.memory import clear_device_cache\n",
        "# del model\n",
        "# del trainer\n",
        "# clear_device_cache()"
      ],
      "metadata": {
        "id": "tzvRwq5gZa23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.set_format('torch')\n",
        "test_data.set_format('torch')"
      ],
      "metadata": {
        "id": "QbDXdU1TdvuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "PnMbaS8yd1X1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-uncased\", num_labels=2, torch_dtype=\"auto\")"
      ],
      "metadata": {
        "id": "nqbz9VBoeN1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, TaskType, get_peft_model"
      ],
      "metadata": {
        "id": "QZkikJ8eeTxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(task_type=TaskType.SEQ_CLS, inference_mode=False, r=8, lora_dropout=0.1)"
      ],
      "metadata": {
        "id": "JSNjWwhTeYbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "Xic4JZ99ehMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer"
      ],
      "metadata": {
        "id": "8SaJDPCGeudu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "hLsAGxIveudv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "F2Lv5M-8eudv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='test_trainer',\n",
        "    eval_strategy=\"epoch\",\n",
        "    num_train_epochs=2,\n",
        "    report_to=\"none\",\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        ")"
      ],
      "metadata": {
        "id": "2e62sBz3e1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=test_data,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "sp2Dnw1Oe1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.train()"
      ],
      "metadata": {
        "id": "eOCwWOMse9Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bjKhs7gse-5B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}